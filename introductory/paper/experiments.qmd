# Experiments

## Random access

Based on experiments in [@zeng2023empirical].

## Scanning

Scanning data is used in both OLAP queries and Data Loader workloads. As a comparison we'll be using Parquet, a common OLAP format, and TFRecord, a common data Loader format.

There are two major optimizations in Lance aimed at vector and unstructured data. The first is late materialization in queries with filters, which can save IO calls. The second are strong alignment of the on-disk format with the in-memory format.

### Overall scan performance

To provide an overall idea of performance while scanning, we compare Lance to Parquet [^parquet-scan-note] for two datasets. One is the first 1.8 million rows of the Laion-5B dataset, including a vector embedding column. The other is the `lineitem` table from the TPC-H dataset (with scale factor 10). The Laion dataset represents a typical computer vision dataset, while the TPC-H lineitem table represents a typical OLAP dataset.

[^parquet-scan-note]: The Parquet implementation we used to scan is PyArrow `15.0.0dev338`. The `lineitem` files were written with DuckDB 0.9.0, using default settings (SNAPPY compression). The Laion dataset was written with PyArrow. The vector column in this dataset was float16 type with 768 dimensions.

The first relevant factor in the scan performance is on-disk size, since this determines how much IO must be performed to scan the full table. The sizes are shown in @fig-dataset-size. In both cases, Lance is larger than Parquet. This is because Lance does not currently implement any encodings or compression which might reduce the on-disk size. This size difference is especially pronounced for the TCP-H dataset, which largely has data types that are highly compressible. The Laion dataset does not benefit nearly as much from compression because much of the dataset size comes from the vector embedding column, which has been found in @zeng2023empirical to be difficult to compress. In fact, if you write just the vector column out to files in Lance and Parquet, they will be roughly the same size.

```{r, warning=FALSE, message=FALSE}
library(gt)
library(ggplot2)
library(tidyr)
library(dplyr)

# data <- read.csv("introductory/experiments/overall_scans/results.csv")
data <- read.csv("../experiments/overall_scans/results.csv")

data$format <- tools::toTitleCase(data$format)
data$dataset <- ifelse(data$dataset == "laion", "Laion", "TPC-H lineitem")
```


```{r}
#| label: fig-dataset-size
#| fig-cap: Size of datasets used in scan benchmarks
data |>
    filter(row_group_size == 102400) |>
    select(dataset, format, dataset_size_bytes) |>
    mutate(dataset_size_bytes = dataset_size_bytes / 1024 / 1024 / 1024) |>
    pivot_wider(names_from = format, values_from = dataset_size_bytes) |>
    mutate(Ratio = Lance / Parquet) |>
    gt(rowname_col = "dataset") |>
    tab_spanner(
        label="On-disk size (GB)",
        columns=c("Lance", "Parquet")
    ) |>
    fmt_number(
        columns = c("Lance", "Parquet", "Ratio"),
        decimals = 1
    )
```

The relative performance of scans in Lance verus Parquet varies between the datasets. For the TPC-H dataset, Lance is roughly 45% slower than Parquet. This is likely simply due to the relative on-disk size. Meanwhile in the Laion dataset, scanning Lance is 5x faster than scanning Parquet, likely due to it's more efficient decoding of the vector column.

It's worth noting that Lance's default setting is for row group size of 1024. This setting reduces scan performance of analytic data significantly, in both Lance and Parquet. This default setting is used since many Lance datasets have some wide columns such as images, where buffering more than 1024 rows before flushing to disk would mean excessive memory overhead. However, given this performance hit for scalar columns, future work may be necesary to decouple the page size of wide and small columns to allow for better OLAP scan performance while still maintaining reasonable memory requirements when writing data.

```{r}
#| label: fig-full-scan-runtime
#| fig-cap: Time to scan full datasets.
#| fig-height: 4
#| fig-width: 5
ggplot(data, aes(x = format, y = scan_time)) +
    geom_col() +
    facet_grid(dataset ~ row_group_size, labeller = "label_both") +
    labs(
      x="Format",
      y="Runtime (s)",
     )
```


### Late materialization

Late materialization can reduce IO costs by deferring the decision whether to load certain cells depending on the result of a filter. This is especially important when the projected columns are large, since the potential IO cost savings are substantial.

Late materialization is an engine optimization, and can be applied to any columnar format. However, the performance benefit of this optimization depends on the page structure of the format. If pages are large and cannot be sliced, then late materialization only will be beneficial to the extent that whole pages can be skipped. In Lance, vector and binary columns are laid out in a flat layout, which can be sliced at the cell-level.

To demonstrate the performance benefit, we measured early versus late materialization strategies in Lance and Parquet. The PyArrow Parquet implementation does not have a built-in late materialization code path, so we created a basic one. Because it is not optimized to be CPU-efficient, we focus on measurements of IO calls and bytes read to give a fair comparison. For Lance and the early-materialization of Parquet, we show the runtime as well.

<!-- Chart 1: line chart. Color: Format, line shape: early vs late,
       x-axis: filter selectivity, y-axis: runtime -->

<!-- Chart 2: line chart. Color: Format, line shape: early vs late,
       x-axis: filter selectivity, y-axis: bytes read -->

<!-- TODO: discuss results -->
