 The open-source file formats have been widely adopted and proliferated for the past 10 years. 
 Apache Haddop first introduced two row-oriented formats, SequenceFile[@sequencefile] and Avro[@avro], which 
 were adopted by Haddop ecosystems. Then Meta released a column-oriented format for Hadoop called RCFile[@rcfile-2011]. 
 Shortly after that, Meta refined RCFile to ORC (Optimized Record Columnar File) format [@orc-2014]. 
 Twitter and Cloudera andannounced the first version of parguet [@parquet] 1 month after that. 
 Ever since then, column-oriented file formats become dominant in the big data ecosystems, 
 and they are supported by most data processing platforms.

 Columnar file formats such as Parquet and ORC present the advantages of efficient data storage and retrieval,
 making them the great candidate for storing big data of any kind, thus becoming the foundation of data platforms [@snowflake-2020, @motherduck-2024]. 
 The data throughput and performance of queries based on such formats is orders of magnitude better, compared to row-oriented formats[@column-vs-rows]
 given techniques such as columnar compression, late materialization, and SIMD-friendly layout. Parquet has rich APIs and can easily work with popular 
 cloud services hosted by AWS, GCP and Azure. Zheng {/it et al.} conducted a comprehensive performance study 
 over columnar file formats [@zeng2023empirical].

 Although Parquet and ORC have the polularity with big data systems, with the fast advancement of AI applications in 
 recent years, they are not suitable due to the fact that AI applications usually require large columns 
 (e.g. embedded vectors, images, sensor data) for storage. For example, the LAION-5B dataset [@laion-5b] is distributed as a set of Parquet files for 
 metadata columns paired with NumPy binary files for the vector embeddings. Such AI applications would expose the following
 particular workloads. First, data loading that essentially requires scanning in a randomized fashion. An example is
 how Petastorm[@petastorm] library enables parquet usage of Tensorflow, PyTorch and other ML frameworks. Second, embedding based
 vector search to build RAG applications, which requires performant search from random access to the underlying data. Research has been active in the area of 
 improving indexing[@col-sketch-2018, @col-imprints-2013, @bitweaving-2013], 
 data compression[@fastlanes-2023, @fsst-2020, @compression-2006, @compression-2019] and more efficient query execution by taking advantage of 
 hardware improvement such as GPU[@simd-2020, @scan-2017, @rethink-2015]. Systems like Artus[@procella2019] and BtrBlocks[@btrblocks] allows reading a single row without 
 decoding a full page, as well as generic compression to accelerate performance. There is still a need to 
 have a new file format which can combine the advancement from all such research efforts. Furthermore,
 it can provide rich APIs and integrate easily with ML frameworks.

 Another group of research goes into learning the column layouts given the workload 
 knowledge and performance requirements[@col-layout-2019], which decides the optimal dataset layout 
 based on the specific workload, e.g. how to physically order data, column density level and buffer 
 space allocation for updates. Google presented Procella [@procella2019] to combine data lake and data 
 analytics system, which poses the challenge of metadata management. 
