# Lance: Efficient Random Access in Columnar Storage through Adaptive Structural Encodings

This folder contains the code neccessary to reproduce the results in the Lance 2.1 paper. The code is organized as follows:

- `figures/`: Contains the hand-crafted figures present in the paper.
- `experiments/`: Contains the code to run the experiments.
- `results/`: Contains CSV results that we obtained from running the experiments and that were used in the paper.
- `scripts/`: Contains scripts to generate the figures from the results.
- `data/`: Contains the data used to run the experiments. These data files are large and must be downloaded separately. See the `data/README.md` file for more information.

## Hardware

We used the following hardware to run the experiments:

- A local desktop with an Intel i7-10700K CPU, 64 GiB of RAM, and a Samsung 970 EVO Plus 2TB NVMe SSD. Further details
  can be found in the file `localhw.info` which contain the results of running `sudo lshw`. The OS used for these experiments
  was Ubuntu 24.04.1 LTS. These experiments were carried out with `rustc 1.83.0 (90b35a623 2024-11-26)`. Any C/C++ compilation
  was done with `clang version 18.1.3`. The protobuf compiler version was `libprotoc 23.4`.

- An EC2 instance was used for S3 tests. The instance was a `c7gn.8xlarge` instance because we wanted to test something with
  a high bandwidth network interface to show the performance of S3. The instance was deployed with the stock Ubuntu 24 AMI.

## Explanations of assertions made in the paper

### Performance characteristics of S3 and NVMe storage

In section 1 we introduce a chart showing the performance characteristics of S3 and NVMe storage. This was generated by
running the `fsprof` experiment in the `experiments/` directory. There is a binary named `local` which will test the local
disk using the standard Rust library and a binary named `s3` which uses `object_store` to test S3.

The results are stored in `results/fsprof_disk_results.csv` and `results/fsprof_s3_results.csv`.

### Baseline disk performance

In section 5 we discuss the baseline disk performance. The script `experiments/random-take/baseline.py` shows how we determined
the peak IOPS that the disk could handle. This script used the builtin `fio` tool to measure the performance of the disk. The
results are stored in `results/baseline.csv`.

There is another baseline that we used for the random access experiments. In this case, instead of measuring the data size at
regular intervals, we measured at the sizes that matched the categories we used in our random access experiments. Otherwise, the
methodology was the same. The script used is `experiments/random-take/take_baseline.py` and the results are stored in
`results/take_baseline.csv`.

The peak throughput number was simpler to measure. We measured this through manually running `fio`, `dd`, and the Gnome disk
utility. All three gave similar results and an approximate peak throughput of 3.4 GB/s which is what we used in the paper.

### Figure 9: Coalesced access

In figure 9 we plot the theoretical benefit of coalesced access at different dataset sizes. This was done using the `page-count`
binary in the `experiments/random-take` directory. This binary simple does a monte carlo simulation of random access to a file.
The results are stored in `results/coalesce.csv`. The chart is generated by the chart script `chart-scripts/coalesce_chart.py`.

## Figure 10, chart 1: Parquet random access performance

In this chart we show the performance of Parquet against different data types. This was done using the `main` binary in the
`experiments/random-take` directory. This binary first writes some number of Parquet files of a given configuration. It then
loads the metadata for these Parquet files into memory. It then performs "take" operations against the Parquet files across
multiple threads, for some number of seconds. Before the experiment starts all files are dropped from the kernel page cache.
However, we cannot afford to drop the cache in between each take as that would disrupt the results. Instead we try and ensure
there is enough data that coalesced benefits are small.

The experiment has many command line arguments. The script `experiments/random-take/benchmark_parquet.py` runs the experiment
for a variety of configurations. The results are stored in `results/take_parquet.csv`. The chart is generated with the chart
script `chart-scripts/take_parquet.py`.

## Section 6.1.1: Effects of dictionary encoding and compression on random access

In the paper we talk about the effects of dictionary encoding and compression on random access. This was done using the `main`
binary in the `experiments/random-take` directory. The script `experiments/random-take/benchmark_parquet_encoding.py` runs the
experiment for a variety of configurations. The results are stored in `results/parquet_encoding.csv`. A chart can be generated
with the chart script `chart-scripts/parquet_encoding.py` but this chart was not used in the paper for space reasons.

## Figure 10, chart 2: Parquet performance across different page sizes

In this chart we show the random access performance of Parquet against different page sizes. This was done using the same
results as we used for figure 10, chart 1. The chart script is `chart-scripts/parquet_page_chart.py`.

## Figure 11, chart 1: Lance random access performance

In this chart we show the random access performance of Lance against different data types. This was done using the `main`
binary in the `experiments/random-take` directory, similar to the Parquet experiment. The script
`experiments/random-take/benchmark_lance.py` runs the experiment for a variety of configurations. The results are stored
in `results/take_lance.csv` and also in `results/take_lance_old.csv`. The chart is generated with the chart script
`chart-scripts/take_lance.py`.

## Figure 11, chart 2: Lance random access at different levels of nesting

In this chart we show the random access performance of Lance against different levels of nesting. This was done using
the `main` binary in the `experiments/random-take` directory. The script `experiments/random-take/benchmark_nesting.py`
runs the experiment for a variety of configurations. The results are stored in `results/nesting.csv`. In addition, we ran
the same experiment on S3 with a few small tweaks to the benchmark script to change the file paths. The results are stored
in `results/nesting_s3.csv`. The chart is generated with the chart script `chart-scripts/nesting.py`.

## Figure 12: Lance random access performance with different structural encodings

In this chart we explore the effects of structural encoding on random access performance in the Lance 2.1 format. This was
done using the `main` binary in the `experiments/random-take` directory. The script `experiments/random-take/benchmark_sized.py`
runs the experiment for a variety of configurations. The results are stored in `results/sized.csv`. The chart is generated
with the chart script `chart-scripts/sized_take.py`.

## Figure 13: Lance and Parquet compression ratios

In this chart we explore the compression ratios of Parquet and the Lance 2.1 format. This was done using the `uncompressed`
binary in the `experiments/random-take` directory. This script reads a hard-coded directory of data files (`data/real`) and
writes a parquet file with no compression, a parquet file with default compression, and a Lance 2.1 file with default compression.
The results are stored in `results/uncompression.csv`. The chart is generated with the chart script `chart-scripts/uncompression_chart.py`.

## Figure 14: Parquet scan performance

In this chart we explore the effectiveness of Parquet at using the NVMe storage. This was done using the `full-scan` binary
in the `experiments/random-take` directory. This script writes Parquet files using a specific configuration from command line
arguments. It then spends some number of seconds attempting to read the files as many times as possible. Before each iteration
we drop the file from the kernel page cache.

The only options we specified were `--quiet`, `--format parquet`, and `--concurrency 32` (we found this had the best performance).
This runs the experiment on all categories across a variety of page sizes and row groups. The experiment will skip the 1024 row
group run against the `dates` category as this would crash. The results are stored in `results/parquet_full_scan.csv`. The chart
script `chart-scripts/parquet_scan_disk.py` generates the chart. There is another chart script `chart-scripts/parquet_scan_page.py`
which helped us explore the effects of page sizes. However, this chart is not included in the paper due to space limitations. Instead
we simply report the numbers.

## Figure 15: Parquet row group effects

In this chart we explore the effects of row group size on Parquet scan performance. This was done using the same results as we used
for figure 14. The chart script is `chart-scripts/parquet_row_group.py`.

## Figure 16, chart 1: Lance vs. Parquet scan performance

In this chart we compare Lance scan performance with Parquet scan performance. This was done using the `full-scan` binary in the
`experiments/random-take` directory. The only options specified were `--quiet`, `--format lance`, and `--concurrency 32`. The experiment
does not need to run at multiple page sizes or row group sizes as those are not configuration options in Lance. The results are stored in
`results/lance_full_scan.csv`. The chart is generated with the chart script `chart-scripts/parquet_vs_lance_scan.py`.

## Figure 16, chart 2: Lance vs. Parquet disk utilization

In this chart we compare the disk utilization of Lance with Parquet. This was done using the same results as we used for figure 16, chart 1.
The chart script is `chart-scripts/lance_scan_disk.py`.

## Figure 17: Lance scan performance with different structural encodings

In this chart we explore the effects of structural encoding on scan performance in the Lance 2.1 format. This was done using the `full-scan`
binary in the `experiments/random-take` directory. In order to test this we generated synthetic data files. The script
`experiments/random-take/prep_sized.py` generates the data files. Some amount of manual work was then done to copy the files to the
`data/synthetic/sizes` directory. This directory was then passed to the `full-scan` binary via command line arguments. The results are
stored in `results/sized_scan.csv`. The chart is generated with the chart script `chart-scripts/sized_scan.py`.

## Figure 18: Struct packing

In this chart we explore Lance 2.1's ability to pack structs. We measure both random access performance and scan performance. The random
access performance is measured using the `main` binary in the `experiments/random-take` directory. The script
`experiments/random-take/benchmark_packed.py` runs the experiment for a variety of configurations. The results are stored in
`results/packed_take.csv`.

The scan performance is measured using the `full-scan` binary in the `experiments/random-take` directory. In order to test this we generated
synthetic data files. The script `experiments/random-take/prep_packed.py` generates the data files. Some amount of manual work was then done
to copy the files to the `data/synthetic/packed` directory. This directory was then passed to the `full-scan` binary via command line arguments.
The results are stored in `results/packed_scan.csv`.

The chart script is `chart-scripts/packed.py`.
